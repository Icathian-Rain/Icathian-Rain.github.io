<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Faster RCNN 源码解析 | IcathianRain's Blog</title><meta name="author" content="IcathianRain"><meta name="copyright" content="IcathianRain"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Pytorch官方FasterRCNN0. 代码结构  FasterRCNN继承GeneralizedRCNN， GeneralizedRCNN继承nn.Module 1. GeneralizedRCNN123456789101112131415161718192021222324252627282930class GeneralizedRCNN(nn.Module):    def __init">
<meta property="og:type" content="article">
<meta property="og:title" content="Faster RCNN 源码解析">
<meta property="og:url" content="https://blog.icathianrain.me/2023/07/26/FasterRCNN/index.html">
<meta property="og:site_name" content="IcathianRain&#39;s Blog">
<meta property="og:description" content="Pytorch官方FasterRCNN0. 代码结构  FasterRCNN继承GeneralizedRCNN， GeneralizedRCNN继承nn.Module 1. GeneralizedRCNN123456789101112131415161718192021222324252627282930class GeneralizedRCNN(nn.Module):    def __init">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://blog.icathianrain.me/images/avatar.jpg">
<meta property="article:published_time" content="2023-07-26T10:46:33.000Z">
<meta property="article:modified_time" content="2023-07-30T04:19:31.700Z">
<meta property="article:author" content="IcathianRain">
<meta property="article:tag" content="AI">
<meta property="article:tag" content="FasterRCNN">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://blog.icathianrain.me/images/avatar.jpg"><link rel="shortcut icon" href="/images/avatar.jpg"><link rel="canonical" href="https://blog.icathianrain.me/2023/07/26/FasterRCNN/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Faster RCNN 源码解析',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-07-30 04:19:31'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="//at.alicdn.com/t/c/font_4179705_k5qmnhfql49.css"><meta name="generator" content="Hexo 6.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/images/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">6</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">7</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">2</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签页</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类页</span></a></div><div class="menus_item"><a class="site-page" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="IcathianRain's Blog"><span class="site-name">IcathianRain's Blog</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签页</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类页</span></a></div><div class="menus_item"><a class="site-page" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">Faster RCNN 源码解析</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-07-26T10:46:33.000Z" title="发表于 2023-07-26 10:46:33">2023-07-26</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-07-30T04:19:31.700Z" title="更新于 2023-07-30 04:19:31">2023-07-30</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Knowledge/">Knowledge</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Faster RCNN 源码解析"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><article class="post-content" id="article-container"><h1 id="Pytorch官方FasterRCNN"><a href="#Pytorch官方FasterRCNN" class="headerlink" title="Pytorch官方FasterRCNN"></a>Pytorch官方FasterRCNN</h1><h1 id="0-代码结构"><a href="#0-代码结构" class="headerlink" title="0. 代码结构"></a>0. 代码结构</h1><img src="/2023/07/26/FasterRCNN/image-20230725160027560.png" class="" title="image-20230725160027560">

<p>FasterRCNN继承GeneralizedRCNN， GeneralizedRCNN继承nn.Module</p>
<h1 id="1-GeneralizedRCNN"><a href="#1-GeneralizedRCNN" class="headerlink" title="1. GeneralizedRCNN"></a>1. GeneralizedRCNN</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">GeneralizedRCNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, backbone, rpn, roi_heads, transform</span>):</span><br><span class="line">        <span class="built_in">super</span>(GeneralizedRCNN, self).__init__()</span><br><span class="line">        self.transform = transform</span><br><span class="line">        self.backbone = backbone</span><br><span class="line">        self.rpn = rpn</span><br><span class="line">        self.roi_heads = roi_heads</span><br><span class="line"></span><br><span class="line">    <span class="comment"># images是输入的除以255归一化后的batch图像</span></span><br><span class="line">    <span class="comment"># targets是输入对应images的batch标记框（如果self.training训练模式，targets不能为空）</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, images, targets=<span class="literal">None</span></span>):</span><br><span class="line">        original_image_sizes = torch.jit.annotate(<span class="type">List</span>[<span class="type">Tuple</span>[<span class="built_in">int</span>, <span class="built_in">int</span>]], [])</span><br><span class="line">        <span class="keyword">for</span> img <span class="keyword">in</span> images:</span><br><span class="line">            val = img.shape[-<span class="number">2</span>:]</span><br><span class="line">            <span class="keyword">assert</span> <span class="built_in">len</span>(val) == <span class="number">2</span></span><br><span class="line">            original_image_sizes.append((val[<span class="number">0</span>], val[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">        images, targets = self.transform(images, targets)</span><br><span class="line">        features = self.backbone(images.tensors)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(features, torch.Tensor):</span><br><span class="line">            features = OrderedDict([(<span class="string">&#x27;0&#x27;</span>, features)])</span><br><span class="line">        proposals, proposal_losses = self.rpn(images, features, targets)</span><br><span class="line">        detections, detector_losses = self.roi_heads(features, proposals, images.image_sizes, targets)</span><br><span class="line">        detections = self.transform.postprocess(detections, images.image_sizes, original_image_sizes)</span><br><span class="line"></span><br><span class="line">        losses = &#123;&#125;</span><br><span class="line">        losses.update(detector_losses)</span><br><span class="line">        losses.update(proposal_losses)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> (losses, detections)</span><br></pre></td></tr></table></figure>

<p>有四个重要接口：</p>
<ul>
<li>transform</li>
<li>backbone</li>
<li>rpn</li>
<li>roi_heads</li>
</ul>
<h2 id="1-1-transform"><a href="#1-1-transform" class="headerlink" title="1.1 transform"></a>1.1 transform</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GeneralizedRCNN.forward(...)</span></span><br><span class="line"><span class="keyword">for</span> img <span class="keyword">in</span> images:</span><br><span class="line">    val = img.shape[-<span class="number">2</span>:]</span><br><span class="line">    <span class="keyword">assert</span> <span class="built_in">len</span>(val) == <span class="number">2</span></span><br><span class="line">    original_image_sizes.append((val[<span class="number">0</span>], val[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">images, targets = self.transform(images, targets)</span><br></pre></td></tr></table></figure>

<ol>
<li>记录每个图片的原始大小，便于把网络输出的缩放框放在原始图像上。</li>
<li>对<code>images</code>, <code>targets</code>进行transform<ol>
<li>将输入标准化( Norm )</li>
<li>将图像缩放到固定大小 ( Resize )。 对于<code>FasterRCNN</code>，从纯理论上来说确实可以支持任意大小的图片。但是若图片过大，会爆内存。</li>
</ol>
</li>
</ol>
<h2 id="1-2-backbone-rpn-roi-heads"><a href="#1-2-backbone-rpn-roi-heads" class="headerlink" title="1.2 backbone+rpn+roi_heads"></a>1.2 backbone+rpn+roi_heads</h2><p>将图像缩放后，进行下面四个步骤：</p>
<ol>
<li><p>将transform 后的图像输入到backbone模块提取特征图.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">features = self.backbone(images.tensors)</span><br><span class="line"><span class="keyword">if</span> <span class="built_in">isinstance</span>(features, torch.Tensor):</span><br><span class="line">	features = OrderedDict([(<span class="string">&#x27;0&#x27;</span>, features)])</span><br></pre></td></tr></table></figure>

<p>backbone一般为VGG，ResNet，MobileNet</p>
</li>
<li><p>经过rpn模块生成propsals和proposal_losses</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">proposals, proposal_losses = self.rpn(images, features, targets)</span><br></pre></td></tr></table></figure>
</li>
<li><p>进入roi_heads模块(即roi_pooling+分类)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">detections, detector_losses = self.roi_heads(features, proposals, images.image_sizes, targets)</span><br></pre></td></tr></table></figure>
</li>
<li><p>最后经过postproces模块（进行NMS，同时将box用过original_images_size映射回原图）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">detections = self.transform.postprocess(detections, images.image_sizes, original_image_sizes)</span><br></pre></td></tr></table></figure></li>
</ol>
<h1 id="2-FasterRCNN"><a href="#2-FasterRCNN" class="headerlink" title="2. FasterRCNN"></a>2. FasterRCNN</h1><p>FasterRCNN继承基类GeneralizedRCNN。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">FasterRCNN</span>(<span class="title class_ inherited__">GeneralizedRCNN</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, backbone, num_classes=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 <span class="comment"># transform parameters</span></span></span><br><span class="line"><span class="params">                 min_size=<span class="number">800</span>, max_size=<span class="number">1333</span>,</span></span><br><span class="line"><span class="params">                 image_mean=<span class="literal">None</span>, image_std=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 <span class="comment"># RPN parameters</span></span></span><br><span class="line"><span class="params">                 rpn_anchor_generator=<span class="literal">None</span>, rpn_head=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 rpn_pre_nms_top_n_train=<span class="number">2000</span>, rpn_pre_nms_top_n_test=<span class="number">1000</span>,</span></span><br><span class="line"><span class="params">                 rpn_post_nms_top_n_train=<span class="number">2000</span>, rpn_post_nms_top_n_test=<span class="number">1000</span>,</span></span><br><span class="line"><span class="params">                 rpn_nms_thresh=<span class="number">0.7</span>,</span></span><br><span class="line"><span class="params">                 rpn_fg_iou_thresh=<span class="number">0.7</span>, rpn_bg_iou_thresh=<span class="number">0.3</span>,</span></span><br><span class="line"><span class="params">                 rpn_batch_size_per_image=<span class="number">256</span>, rpn_positive_fraction=<span class="number">0.5</span>,</span></span><br><span class="line"><span class="params">                 <span class="comment"># Box parameters</span></span></span><br><span class="line"><span class="params">                 box_roi_pool=<span class="literal">None</span>, box_head=<span class="literal">None</span>, box_predictor=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 box_score_thresh=<span class="number">0.05</span>, box_nms_thresh=<span class="number">0.5</span>, box_detections_per_img=<span class="number">100</span>,</span></span><br><span class="line"><span class="params">                 box_fg_iou_thresh=<span class="number">0.5</span>, box_bg_iou_thresh=<span class="number">0.5</span>,</span></span><br><span class="line"><span class="params">                 box_batch_size_per_image=<span class="number">512</span>, box_positive_fraction=<span class="number">0.25</span>,</span></span><br><span class="line"><span class="params">                 bbox_reg_weights=<span class="literal">None</span></span>):</span><br><span class="line">		<span class="comment"># backbone</span></span><br><span class="line">        out_channels = backbone.out_channels</span><br><span class="line">		<span class="comment"># rpn</span></span><br><span class="line">        <span class="keyword">if</span> rpn_anchor_generator <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            anchor_sizes = ((<span class="number">32</span>,), (<span class="number">64</span>,), (<span class="number">128</span>,), (<span class="number">256</span>,), (<span class="number">512</span>,))</span><br><span class="line">            aspect_ratios = ((<span class="number">0.5</span>, <span class="number">1.0</span>, <span class="number">2.0</span>),) * <span class="built_in">len</span>(anchor_sizes)</span><br><span class="line">            rpn_anchor_generator = AnchorGenerator(</span><br><span class="line">                anchor_sizes, aspect_ratios</span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">if</span> rpn_head <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            rpn_head = RPNHead(</span><br><span class="line">                out_channels, rpn_anchor_generator.num_anchors_per_location()[<span class="number">0</span>]</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        rpn_pre_nms_top_n = <span class="built_in">dict</span>(training=rpn_pre_nms_top_n_train, testing=rpn_pre_nms_top_n_test)</span><br><span class="line">        rpn_post_nms_top_n = <span class="built_in">dict</span>(training=rpn_post_nms_top_n_train, testing=rpn_post_nms_top_n_test)</span><br><span class="line">	</span><br><span class="line">        rpn = RegionProposalNetwork(</span><br><span class="line">            rpn_anchor_generator, rpn_head,</span><br><span class="line">            rpn_fg_iou_thresh, rpn_bg_iou_thresh,</span><br><span class="line">            rpn_batch_size_per_image, rpn_positive_fraction,</span><br><span class="line">            rpn_pre_nms_top_n, rpn_post_nms_top_n, rpn_nms_thresh)</span><br><span class="line">		<span class="comment"># roi</span></span><br><span class="line">        <span class="keyword">if</span> box_roi_pool <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            box_roi_pool = MultiScaleRoIAlign(</span><br><span class="line">                featmap_names=[<span class="string">&#x27;0&#x27;</span>, <span class="string">&#x27;1&#x27;</span>, <span class="string">&#x27;2&#x27;</span>, <span class="string">&#x27;3&#x27;</span>],</span><br><span class="line">                output_size=<span class="number">7</span>,</span><br><span class="line">                sampling_ratio=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> box_head <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            resolution = box_roi_pool.output_size[<span class="number">0</span>]</span><br><span class="line">            representation_size = <span class="number">1024</span></span><br><span class="line">            box_head = TwoMLPHead(</span><br><span class="line">                out_channels * resolution ** <span class="number">2</span>,</span><br><span class="line">                representation_size)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> box_predictor <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            representation_size = <span class="number">1024</span></span><br><span class="line">            box_predictor = FastRCNNPredictor(</span><br><span class="line">                representation_size,</span><br><span class="line">                num_classes)</span><br><span class="line"></span><br><span class="line">        roi_heads = RoIHeads(</span><br><span class="line">            <span class="comment"># Box</span></span><br><span class="line">            box_roi_pool, box_head, box_predictor,</span><br><span class="line">            box_fg_iou_thresh, box_bg_iou_thresh,</span><br><span class="line">            box_batch_size_per_image, box_positive_fraction,</span><br><span class="line">            bbox_reg_weights,</span><br><span class="line">            box_score_thresh, box_nms_thresh, box_detections_per_img)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> image_mean <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            image_mean = [<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>]</span><br><span class="line">        <span class="keyword">if</span> image_std <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            image_std = [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>]</span><br><span class="line">        transform = GeneralizedRCNNTransform(min_size, max_size, image_mean, image_std)</span><br><span class="line"></span><br><span class="line">        <span class="built_in">super</span>(FasterRCNN, self).__init__(backbone, rpn, roi_heads, transform)</span><br></pre></td></tr></table></figure>

<p>FasterRCNN 实现了 GeneralizedRCNN 中的 transform、backbone、rpn、roi_heads 接口：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># FasterRCNN.__init__(...)</span></span><br><span class="line"><span class="built_in">super</span>(FasterRCNN, self).__init__(backbone, rpn, roi_heads, transform)</span><br></pre></td></tr></table></figure>

<h2 id="2-1-transform-接口"><a href="#2-1-transform-接口" class="headerlink" title="2.1 transform 接口"></a>2.1 transform 接口</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">transform = GeneralizedRCNNTransform(min_size, max_size, image_mean, image_std)</span><br></pre></td></tr></table></figure>

<p>使用 GeneralizedRCNNTransform 实现。从代码变量名可以明显看到包含：</p>
<ul>
<li>与缩放有关的参数：：min_size + max_size</li>
<li>与归一化有关的参数: image_mean + image_std（对输入[0, 1]减去image_mean再除以image_std）</li>
</ul>
<h2 id="2-2-backbone-接口"><a href="#2-2-backbone-接口" class="headerlink" title="2.2 backbone 接口"></a>2.2 backbone 接口</h2><p>使用 ResNet50 + FPN(resnet_fpn_backbone) 结构：</p>
<p>在生成model前，通过backbone将参数传入</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">fasterrcnn_resnet50_fpn</span>(<span class="params">pretrained=<span class="literal">False</span>, progress=<span class="literal">True</span>, num_classes=<span class="number">91</span>, pretrained_backbone=<span class="literal">True</span>, **kwargs</span>):</span><br><span class="line">    <span class="keyword">if</span> pretrained:</span><br><span class="line">        <span class="comment"># no need to download the backbone if pretrained is set</span></span><br><span class="line">        pretrained_backbone = <span class="literal">False</span></span><br><span class="line">    backbone = resnet_fpn_backbone(<span class="string">&#x27;resnet50&#x27;</span>, pretrained_backbone)</span><br><span class="line">    model = FasterRCNN(backbone, num_classes, **kwargs)</span><br><span class="line">    <span class="keyword">if</span> pretrained:</span><br><span class="line">        state_dict = load_state_dict_from_url(model_urls[<span class="string">&#x27;fasterrcnn_resnet50_fpn_coco&#x27;</span>], progress=progress)</span><br><span class="line">        model.load_state_dict(state_dict)</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>

<h3 id="2-2-1-ResNet50"><a href="#2-2-1-ResNet50" class="headerlink" title="2.2.1 ResNet50"></a>2.2.1 ResNet50</h3><h3 id="2-2-2-FPN-Feature-Pyramid-Network"><a href="#2-2-2-FPN-Feature-Pyramid-Network" class="headerlink" title="2.2.2 FPN (Feature Pyramid Network)"></a>2.2.2 FPN (Feature Pyramid Network)</h3><h4 id="1-动机"><a href="#1-动机" class="headerlink" title="1. 动机"></a>1. 动机</h4><p>识别不同尺寸的物体是目标检测中的一个基本挑战，而特征金字塔一直是多尺度目标检测中的一个基本的组成部分，但是由于特征金字塔计算量大，会拖慢整个检测速度，所以大多数方法为了检测速度而尽可能的去避免使用特征金字塔，而是只使用高层的特征来进行预测。高层的特征虽然包含了丰富的语义信息，但是由于低分辨率，很难准确地保存物体的位置信息。与之相反，低层的特征虽然语义信息较少，但是由于分辨率高，就可以准确地包含物体位置信息。所以如果可以将低层的特征和高层的特征融合起来，就能得到一个识别和定位都准确的目标检测系统。所以本文就旨在设计出这样的一个结构来使得检测准确且快速。</p>
<h4 id="2-结构"><a href="#2-结构" class="headerlink" title="2.结构"></a>2.结构</h4><p>传统的结构包含以下三种</p>
<img src="/2023/07/26/FasterRCNN/image-20230725170819545.png" class="" title="image-20230725170819545">

<ul>
<li>Featurized image pyramid：将图片转换成不同尺寸的，对不同尺寸提取特征，再对每个尺度的特征进行单独预测，优点是：不同尺度的都可以包含丰富的语义信息，但是缺点就是时间成本太高。</li>
<li>Pyramid feature hierarchy：SSD采用的多尺度融合的方法，即从网络不同层抽取不同尺度的特征，然后在这不同尺度的特征上分别进行预测。优点：在于它不需要额外的计算量。缺点：有些尺度的特征语义信息不是很丰富。此外，SSD没有用到足够低层的特征，作者认为低层的特征对于小物体检测是非常有帮助的。</li>
<li>Single feature map：这是在SPPnet，Fast R-CNN，Faster R-CNN中使用的，就是在网络的最后一层的特征图上进行预测。优点：计算速度会比较快。缺点：最后一层的特征图分辨率低，不能准确的包含物体的位置信息。</li>
</ul>
<p>为了使得不同尺度的特征都含丰富的语义信息，同时又不使得计算成本过高。作者采用了top down 和 lateral connection的方式，让低层高分辨率低语义的特征和高层低分辨率高语义的特征融合在一起，使得最终得到的不同尺度的特征图都有丰富的语义信息。</p>
<img src="/2023/07/26/FasterRCNN/image-20230725171328130.png" class="" title="image-20230725171328130">

<h4 id="3-特征金字塔"><a href="#3-特征金字塔" class="headerlink" title="3. 特征金字塔"></a>3. 特征金字塔</h4><img src="https://pic4.zhimg.com/v2-a81f97b300305ec60696f77d9ea8cbf7_b.jpg" data-rawwidth="691" data-rawheight="408" data-size="normal" class="origin_image zh-lightbox-thumb" width="691" data-original="https://pic4.zhimg.com/v2-a81f97b300305ec60696f77d9ea8cbf7_r.jpg"/>

<p>包含三个部分：</p>
<ul>
<li>bottom-up</li>
<li>top-down</li>
<li>lateral connection</li>
</ul>
<p>buttom-up: 将图片输入到backbone ConvNet中提取特征的过程，Backbone输出的feature map的尺寸有的是不变的，有的是成2倍的减小的。对于那些输出的尺寸不变的层，归为一个stage。每个stage的最后一层输出的特征被抽取出来。</p>
<p>以ResNet为例，将卷积块conv2, conv3, conv4, conv5的输出定义为{C2, C3, C4, C5}，这些输出都是stage中最后一个残差块的输出。分别是原图的{1&#x2F;4, 1&#x2F;8, 1&#x2F;16, 1&#x2F;32}。</p>
<p>Top-down: 将高层得到的feature map进行上采样然后往下传递。层的特征包含丰富的语义信息，经过top-down的传播就能使得这些语义信息传播到低层特征上，使得低层特征也包含丰富的语义信息。</p>
<p>采样方法：最近邻上采样，使得特征图扩大2倍。上采样的目的就是放大图片，在原有图像像素的基础上在像素点之间采用合适的插值算法插入新的像素，在本文中使用的是最近邻上采样(插值)。如下图所示：</p>
<img src="https://pic2.zhimg.com/v2-124aa77a5dbae6aa62f9c4076104d2a1_b.jpg" data-rawwidth="325" data-rawheight="230" data-size="normal" class="content_image" width="325"/>

<p>待求像素坐标为(i+u, j+v), 则像素灰度值f(i+u, j+v) &#x3D; a(u&lt;0.5, v&lt;0.5),或b或c或d</p>
<p>Lateral connection</p>
<img src="https://pic2.zhimg.com/v2-066baa36f4824bd7fea639d9dd636e89_b.jpg" data-rawwidth="490" data-rawheight="187" data-size="normal" class="origin_image zh-lightbox-thumb" width="490" data-original="https://pic2.zhimg.com/v2-066baa36f4824bd7fea639d9dd636e89_r.jpg"/>

<ol>
<li>对于每个stage输出的feater map Cn, 进行一个1X1的卷积降低维度</li>
<li>得到的特征与上一层采样的Pn+1进行融合，element-wise addition。因为每个stage输出的特征图之间是2倍的关系，所以上一层上采样得到的特征图的大小和本层的大小一样，就可以直接将对应元素相加 </li>
<li>相加完之后进行一个3X3的卷积，得到本层的特征输出Pn, 使用这个3<em>3卷积的目的是为了消除上采样产生的混叠效应(aliasing effect)，混叠效应应该就是指上边提到的‘插值生成的图像灰度不连续，在灰度变化的地方可能出现明显的锯齿状’。在本文中，因为金字塔所有层的输出特征都共享classifiers&#x2F; regressors，所以输出的维度都被统一为256，即这些3</em>3的卷积的channel都为256。</li>
</ol>
<h2 id="2-3-rpn"><a href="#2-3-rpn" class="headerlink" title="2.3 rpn"></a>2.3 rpn</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> rpn_anchor_generator <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">           anchor_sizes = ((<span class="number">32</span>,), (<span class="number">64</span>,), (<span class="number">128</span>,), (<span class="number">256</span>,), (<span class="number">512</span>,))</span><br><span class="line">           aspect_ratios = ((<span class="number">0.5</span>, <span class="number">1.0</span>, <span class="number">2.0</span>),) * <span class="built_in">len</span>(anchor_sizes)</span><br><span class="line">           rpn_anchor_generator = AnchorGenerator(</span><br><span class="line">               anchor_sizes, aspect_ratios</span><br><span class="line">           )</span><br><span class="line">       <span class="keyword">if</span> rpn_head <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">           rpn_head = RPNHead(</span><br><span class="line">               out_channels, rpn_anchor_generator.num_anchors_per_location()[<span class="number">0</span>]</span><br><span class="line">           )</span><br><span class="line"></span><br><span class="line">       rpn_pre_nms_top_n = <span class="built_in">dict</span>(training=rpn_pre_nms_top_n_train, testing=rpn_pre_nms_top_n_test)</span><br><span class="line">       rpn_post_nms_top_n = <span class="built_in">dict</span>(training=rpn_post_nms_top_n_train, testing=rpn_post_nms_top_n_test)</span><br><span class="line"></span><br><span class="line">       rpn = RegionProposalNetwork(</span><br><span class="line">           rpn_anchor_generator, rpn_head,</span><br><span class="line">           rpn_fg_iou_thresh, rpn_bg_iou_thresh,</span><br><span class="line">           rpn_batch_size_per_image, rpn_positive_fraction,</span><br><span class="line">           rpn_pre_nms_top_n, rpn_post_nms_top_n, rpn_nms_thresh)</span><br></pre></td></tr></table></figure>

<h3 id="2-3-1-rpn-anchor-generator"><a href="#2-3-1-rpn-anchor-generator" class="headerlink" title="2.3.1 rpn_anchor_generator"></a>2.3.1 rpn_anchor_generator</h3><p>对于普通的 FasterRCNN 只需要将 feature_map 输入到 rpn 网络生成 proposals 即可。但是由于加入 FPN，需要将多个 feature_map 逐个输入到 rpn 网络。</p>
<img src="/2023/07/26/FasterRCNN/image-20230725174603657.png" class="" title="image-20230725174603657">

<p>AnchorGenerator的实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">AnchorGenerator</span>(nn.Module):</span><br><span class="line">        ......</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">generate_anchors</span>(<span class="params">self, scales, aspect_ratios, dtype=torch.float32, device=<span class="string">&quot;cpu&quot;</span></span>):</span><br><span class="line">        <span class="comment"># type: (<span class="type">List</span>[<span class="built_in">int</span>], <span class="type">List</span>[<span class="built_in">float</span>], <span class="built_in">int</span>, Device)  # noqa: F821</span></span><br><span class="line">        scales = torch.as_tensor(scales, dtype=dtype, device=device)</span><br><span class="line">        aspect_ratios = torch.as_tensor(aspect_ratios, dtype=dtype, device=device)</span><br><span class="line">        h_ratios = torch.sqrt(aspect_ratios)</span><br><span class="line">        w_ratios = <span class="number">1</span> / h_ratios</span><br><span class="line"></span><br><span class="line">        ws = (w_ratios[:, <span class="literal">None</span>] * scales[<span class="literal">None</span>, :]).view(-<span class="number">1</span>)</span><br><span class="line">        hs = (h_ratios[:, <span class="literal">None</span>] * scales[<span class="literal">None</span>, :]).view(-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        base_anchors = torch.stack([-ws, -hs, ws, hs], dim=<span class="number">1</span>) / <span class="number">2</span></span><br><span class="line">        <span class="keyword">return</span> base_anchors.<span class="built_in">round</span>()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">set_cell_anchors</span>(<span class="params">self, dtype, device</span>):</span><br><span class="line">        <span class="comment"># type: (<span class="built_in">int</span>, Device) -&gt; <span class="literal">None</span>    # noqa: F821</span></span><br><span class="line">        ......</span><br><span class="line"></span><br><span class="line">        cell_anchors = [</span><br><span class="line">            self.generate_anchors(</span><br><span class="line">                sizes,</span><br><span class="line">                aspect_ratios,</span><br><span class="line">                dtype,</span><br><span class="line">                device</span><br><span class="line">            )</span><br><span class="line">            <span class="keyword">for</span> sizes, aspect_ratios <span class="keyword">in</span> <span class="built_in">zip</span>(self.sizes, self.aspect_ratios)</span><br><span class="line">        ]</span><br><span class="line">        self.cell_anchors = cell_anchors</span><br></pre></td></tr></table></figure>

<p>每个位置有5种<code>anchor_size</code>, 3种<code>aspect_radios</code>, 所以每个位置生成15个<code>base_anchors</code>。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[ -23.,  -11.,   23.,   11.] # w = h = 32,  ratio = 2</span><br><span class="line">[ -16.,  -16.,   16.,   16.] # w = h = 32,  ratio = 1</span><br><span class="line">[ -11.,  -23.,   11.,   23.] # w = h = 32,  ratio = 0.5</span><br><span class="line">[ -45.,  -23.,   45.,   23.] # w = h = 64,  ratio = 2</span><br><span class="line">[ -32.,  -32.,   32.,   32.] # w = h = 64,  ratio = 1</span><br><span class="line">[ -23.,  -45.,   23.,   45.] # w = h = 64,  ratio = 0.5</span><br><span class="line">[ -91.,  -45.,   91.,   45.] # w = h = 128, ratio = 2</span><br><span class="line">[ -64.,  -64.,   64.,   64.] # w = h = 128, ratio = 1</span><br><span class="line">[ -45.,  -91.,   45.,   91.] # w = h = 128, ratio = 0.5</span><br><span class="line">[-181.,  -91.,  181.,   91.] # w = h = 256, ratio = 2</span><br><span class="line">[-128., -128.,  128.,  128.] # w = h = 256, ratio = 1</span><br><span class="line">[ -91., -181.,   91.,  181.] # w = h = 256, ratio = 0.5</span><br><span class="line">[-362., -181.,  362.,  181.] # w = h = 512, ratio = 2</span><br><span class="line">[-256., -256.,  256.,  256.] # w = h = 512, ratio = 1</span><br><span class="line">[-181., -362.,  181.,  362.] # w = h = 512, ratio = 0.5</span><br></pre></td></tr></table></figure>

<p>base_anchors如下图所示：</p>
<img src="/2023/07/26/FasterRCNN/image-20230726092820447.png" class="" title="image-20230726092820447">

<p>接着来看 AnchorGenerator.grid_anchors 函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># AnchorGenerator</span></span><br><span class="line">   <span class="keyword">def</span> <span class="title function_">grid_anchors</span>(<span class="params">self, grid_sizes, strides</span>):</span><br><span class="line">        <span class="comment"># type: (<span class="type">List</span>[<span class="type">List</span>[<span class="built_in">int</span>]], <span class="type">List</span>[<span class="type">List</span>[Tensor]])</span></span><br><span class="line">        anchors = []</span><br><span class="line">        cell_anchors = self.cell_anchors</span><br><span class="line">        <span class="keyword">assert</span> cell_anchors <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> size, stride, base_anchors <span class="keyword">in</span> <span class="built_in">zip</span>(</span><br><span class="line">            grid_sizes, strides, cell_anchors</span><br><span class="line">        ):</span><br><span class="line">            grid_height, grid_width = size</span><br><span class="line">            stride_height, stride_width = stride</span><br><span class="line">            device = base_anchors.device</span><br><span class="line"></span><br><span class="line">            <span class="comment"># For output anchor, compute [x_center, y_center, x_center, y_center]</span></span><br><span class="line">            shifts_x = torch.arange(</span><br><span class="line">                <span class="number">0</span>, grid_width, dtype=torch.float32, device=device</span><br><span class="line">            ) * stride_width</span><br><span class="line">            shifts_y = torch.arange(</span><br><span class="line">                <span class="number">0</span>, grid_height, dtype=torch.float32, device=device</span><br><span class="line">            ) * stride_height</span><br><span class="line">            shift_y, shift_x = torch.meshgrid(shifts_y, shifts_x)</span><br><span class="line">            shift_x = shift_x.reshape(-<span class="number">1</span>)</span><br><span class="line">            shift_y = shift_y.reshape(-<span class="number">1</span>)</span><br><span class="line">            shifts = torch.stack((shift_x, shift_y, shift_x, shift_y), dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># For every (base anchor, output anchor) pair,</span></span><br><span class="line">            <span class="comment"># offset each zero-centered base anchor by the center of the output anchor.</span></span><br><span class="line">            anchors.append(</span><br><span class="line">                (shifts.view(-<span class="number">1</span>, <span class="number">1</span>, <span class="number">4</span>) + base_anchors.view(<span class="number">1</span>, -<span class="number">1</span>, <span class="number">4</span>)).reshape(-<span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> anchors</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, image_list, feature_maps</span>):</span><br><span class="line">        <span class="comment"># type: (ImageList, <span class="type">List</span>[Tensor])</span></span><br><span class="line">        grid_sizes = <span class="built_in">list</span>([feature_map.shape[-<span class="number">2</span>:] <span class="keyword">for</span> feature_map <span class="keyword">in</span> feature_maps])</span><br><span class="line">        image_size = image_list.tensors.shape[-<span class="number">2</span>:]</span><br><span class="line">        dtype, device = feature_maps[<span class="number">0</span>].dtype, feature_maps[<span class="number">0</span>].device</span><br><span class="line">        strides = [[torch.tensor(image_size[<span class="number">0</span>] / g[<span class="number">0</span>], dtype=torch.int64, device=device),</span><br><span class="line">                    torch.tensor(image_size[<span class="number">1</span>] / g[<span class="number">1</span>], dtype=torch.int64, device=device)] <span class="keyword">for</span> g <span class="keyword">in</span> grid_sizes]</span><br><span class="line">        self.set_cell_anchors(dtype, device)</span><br><span class="line">        anchors_over_all_feature_maps = self.cached_grid_anchors(grid_sizes, strides)</span><br><span class="line">        ......</span><br></pre></td></tr></table></figure>

<p>因为存在FPN网络，输入rpn的是多个特征。以下对一个特征进行描述，其他特征相似。</p>
<p>假设有h X w的特征，首先会计算每个特征相对于图像的下采样倍数stride（特征相当于原图放缩的倍数）: </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">stride = image_size / feature_map_size</span><br></pre></td></tr></table></figure>

<p>然后生成一个h X w 大小的网格，每个格子长度为stride, 如下图</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># AnchorGenerator.grid_anchors(...)</span></span><br><span class="line">shifts_x = torch.arange(<span class="number">0</span>, grid_width, dtype=torch.float32, device=device) * stride_width</span><br><span class="line">shifts_y = torch.arange(<span class="number">0</span>, grid_height, dtype=torch.float32, device=device) * stride_height</span><br><span class="line">shift_y, shift_x = torch.meshgrid(shifts_y, shifts_x)</span><br></pre></td></tr></table></figure>

<img src="/2023/07/26/FasterRCNN/image-20230726093725882.png" class="" title="image-20230726093725882">

<p>然后将 base_anchors 的中心从 (0,0) 移动到网格的点，且在网格的每个点都放置一组 base_anchors。这样就在当前 feature_map 上有了很多的 anchors。</p>
<p>需要特别说明，stride 代表网络的感受野，网络不可能检测到比 feature_map 更密集的框了！所以才只会在网格中每个点设置 anchors（反过来说，如果在网格的两个点之间设置 anchors，那么就对应 feature_map 中半个点，显然不合理）。</p>
<img src="/2023/07/26/FasterRCNN/image-20230726094716838.png" class="" title="image-20230726094716838">

<h3 id="2-3-2-RPNHead"><a href="#2-3-2-RPNHead" class="headerlink" title="2.3.2 RPNHead"></a>2.3.2 RPNHead</h3><p>放置好 anchors 后，接下来就要调整网络，使网络输出能够判断每个 anchor 是否有目标，同时还要有 bounding box regression 需要的4个值 （dx, dy, dw, dh)。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">RPNHead</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, num_anchors</span>):</span><br><span class="line">        <span class="built_in">super</span>(RPNHead, self).__init__()</span><br><span class="line">        self.conv = nn.Conv2d(</span><br><span class="line">            in_channels, in_channels, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span></span><br><span class="line">        )</span><br><span class="line">        self.cls_logits = nn.Conv2d(in_channels, num_anchors, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>)</span><br><span class="line">        self.bbox_pred = nn.Conv2d(</span><br><span class="line">            in_channels, num_anchors * <span class="number">4</span>, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        logits = []</span><br><span class="line">        bbox_reg = []</span><br><span class="line">        <span class="keyword">for</span> feature <span class="keyword">in</span> x:</span><br><span class="line">            t = F.relu(self.conv(feature))</span><br><span class="line">            logits.append(self.cls_logits(t))</span><br><span class="line">            bbox_reg.append(self.bbox_pred(t))</span><br><span class="line">        <span class="keyword">return</span> logits, bbox_reg</span><br></pre></td></tr></table></figure>

<p>假设 feature 的大小 NCHW ，每个点 k 个 anchor。从 RPNHead 的代码中可以明显看到</p>
<ol>
<li>对每个feature进行3X3的卷积</li>
<li>对feature进行1X1卷积，输出输出 cls_logits 大小是 NkHW ，对应每个 anchor 是否有目标</li>
<li>对feature进行1X1卷积，输出bbox_pred大小是N(4k)HW，对应每个点的4个框位置回归信息(dx, dy, dw, dh)</li>
</ol>
<img src="/2023/07/26/FasterRCNN/image-20230726095337695.png" class="" title="image-20230726095337695">

<p>上述过程只是单个 feature_map 的处理流程。对于 FPN 网络的输出的多个大小不同的 feature_maps，每个特征图都会按照上述过程计算 stride 和网格，并设置 anchors。当处理完后获得密密麻麻的各种 anchors 了。</p>
<h3 id="2-3-3-RegionProposalNetwork"><a href="#2-3-3-RegionProposalNetwork" class="headerlink" title="2.3.3 RegionProposalNetwork"></a>2.3.3 RegionProposalNetwork</h3><p>接下来进入 RegionProposalNetwork 类：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># FasterRCNN.__init__(...)</span></span><br><span class="line">rpn_pre_nms_top_n = <span class="built_in">dict</span>(training=rpn_pre_nms_top_n_train, testing=rpn_pre_nms_top_n_test)</span><br><span class="line">rpn_post_nms_top_n = <span class="built_in">dict</span>(training=rpn_post_nms_top_n_train, testing=rpn_post_nms_top_n_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># rpn_anchor_generator 生成anchors</span></span><br><span class="line"><span class="comment"># rpn_head 调整feature_map获得cls_logits+bbox_pred</span></span><br><span class="line">rpn = RegionProposalNetwork(</span><br><span class="line">    rpn_anchor_generator, rpn_head,</span><br><span class="line">    rpn_fg_iou_thresh, rpn_bg_iou_thresh,</span><br><span class="line">    rpn_batch_size_per_image, rpn_positive_fraction,</span><br><span class="line">    rpn_pre_nms_top_n, rpn_post_nms_top_n, rpn_nms_thresh)</span><br></pre></td></tr></table></figure>

<p>RegionProposalNetwork 类的用是：</p>
<ul>
<li>test 阶段 ：计算有目标的 anchor 并进行框回归生成 proposals，然后 NMS</li>
<li>train 阶段 ：除了上面的作用，还计算 rpn loss</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">RegionProposalNetwork</span>(torch.nn.Module):</span><br><span class="line">    .......</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, images, features, targets=<span class="literal">None</span></span>):</span><br><span class="line">        features = <span class="built_in">list</span>(features.values())</span><br><span class="line">        objectness, pred_bbox_deltas = self.head(features)</span><br><span class="line">        anchors = self.anchor_generator(images, features)</span><br><span class="line"></span><br><span class="line">        num_images = <span class="built_in">len</span>(anchors)</span><br><span class="line">        num_anchors_per_level_shape_tensors = [o[<span class="number">0</span>].shape <span class="keyword">for</span> o <span class="keyword">in</span> objectness]</span><br><span class="line">        num_anchors_per_level = [s[<span class="number">0</span>] * s[<span class="number">1</span>] * s[<span class="number">2</span>] <span class="keyword">for</span> s <span class="keyword">in</span> num_anchors_per_level_shape_tensors]</span><br><span class="line">        objectness, pred_bbox_deltas = \</span><br><span class="line">            concat_box_prediction_layers(objectness, pred_bbox_deltas)</span><br><span class="line">        <span class="comment"># apply pred_bbox_deltas to anchors to obtain the decoded proposals</span></span><br><span class="line">        <span class="comment"># note that we detach the deltas because Faster R-CNN do not backprop through</span></span><br><span class="line">        <span class="comment"># the proposals</span></span><br><span class="line">        proposals = self.box_coder.decode(pred_bbox_deltas.detach(), anchors)</span><br><span class="line">        proposals = proposals.view(num_images, -<span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line">        boxes, scores = self.filter_proposals(proposals, objectness, images.image_sizes, num_anchors_per_level)</span><br><span class="line"></span><br><span class="line">        losses = &#123;&#125;</span><br><span class="line">        <span class="keyword">if</span> self.training:</span><br><span class="line">            <span class="keyword">assert</span> targets <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line">            labels, matched_gt_boxes = self.assign_targets_to_anchors(anchors, targets)</span><br><span class="line">            regression_targets = self.box_coder.encode(matched_gt_boxes, anchors)</span><br><span class="line">            loss_objectness, loss_rpn_box_reg = self.compute_loss(</span><br><span class="line">                objectness, pred_bbox_deltas, labels, regression_targets)</span><br><span class="line">            losses = &#123;</span><br><span class="line">                <span class="string">&quot;loss_objectness&quot;</span>: loss_objectness,</span><br><span class="line">                <span class="string">&quot;loss_rpn_box_reg&quot;</span>: loss_rpn_box_reg,</span><br><span class="line">            &#125;</span><br><span class="line">        <span class="keyword">return</span> boxes, losses</span><br></pre></td></tr></table></figure>

<p>具体来看首先，计算有目标的anchor并进行框回归生成proposals</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">objectness, pred_bbox_deltas = self.head(features)</span><br><span class="line">anchors = self.anchor_generator(images, features)</span><br><span class="line">......</span><br><span class="line">proposals = self.box_coder.decode(pred_bbox_deltas.detach(), anchors)</span><br><span class="line">proposals = proposals.view(num_images, -<span class="number">1</span>, <span class="number">4</span>)</span><br></pre></td></tr></table></figure>

<p>然后依照 objectness 置信由大到小度排序（优先提取更可能包含目标的的），并 NMS，生成 boxes （即 NMS 后的 proposal boxes ） ：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># RegionProposalNetwork.forward(...)</span></span><br><span class="line">boxes, scores = self.filter_proposals(proposals, objectness, images.image_sizes, num_anchors_per_level)</span><br></pre></td></tr></table></figure>

<p>如果是训练阶段，还要将 boxes 与 anchors 进行匹配，计算 cls_logits 的损失 loss_objectness，同时计算 bbox_pred 的损失 loss_rpn_box_reg。</p>
<h2 id="2-4-roi-heads"><a href="#2-4-roi-heads" class="headerlink" title="2.4 roi_heads"></a>2.4 roi_heads</h2><p>在 RegionProposalNetwork 之后已经生成了 boxes ，接下来就要提取 boxes 内的特征进行 roi_pooling ：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">roi_heads = RoIHeads(</span><br><span class="line">    <span class="comment"># Box</span></span><br><span class="line">    box_roi_pool, box_head, box_predictor,</span><br><span class="line">    box_fg_iou_thresh, box_bg_iou_thresh,</span><br><span class="line">    box_batch_size_per_image, box_positive_fraction,</span><br><span class="line">    bbox_reg_weights,</span><br><span class="line">    box_score_thresh, box_nms_thresh, box_detections_per_img)</span><br></pre></td></tr></table></figure>

<h3 id="2-4-1-计算box归属的feature-map"><a href="#2-4-1-计算box归属的feature-map" class="headerlink" title="2.4.1 计算box归属的feature_map"></a>2.4.1 计算box归属的feature_map</h3><p>这里一点问题是如何计算 box 所属的 feature_map：</p>
<ul>
<li>对于原始 FasterRCNN，只在 backbone 的最后一层 feature_map 提取 box 对应特征；</li>
<li>当加入 FPN 后 backbone 会输出多个特征图，由于RPN对anchor进行了box regression后改变了box的大小，所以此时需要重新计算当前 boxes 对应于哪一个特征。</li>
</ul>
<img src="/2023/07/26/FasterRCNN/image-20230726100700452.png" class="" title="image-20230726100700452">

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MultiScaleRoIAlign</span>(nn.Module):</span><br><span class="line">   ......</span><br><span class="line"></span><br><span class="line">   <span class="keyword">def</span> <span class="title function_">infer_scale</span>(<span class="params">self, feature, original_size</span>):</span><br><span class="line">        <span class="comment"># type: (Tensor, <span class="type">List</span>[<span class="built_in">int</span>])</span></span><br><span class="line">        <span class="comment"># assumption: the scale is of the form 2 ** (-k), with k integer</span></span><br><span class="line">        size = feature.shape[-<span class="number">2</span>:]</span><br><span class="line">        possible_scales = torch.jit.annotate(<span class="type">List</span>[<span class="built_in">float</span>], [])</span><br><span class="line">        <span class="keyword">for</span> s1, s2 <span class="keyword">in</span> <span class="built_in">zip</span>(size, original_size):</span><br><span class="line">            approx_scale = <span class="built_in">float</span>(s1) / <span class="built_in">float</span>(s2)</span><br><span class="line">            scale = <span class="number">2</span> ** <span class="built_in">float</span>(torch.tensor(approx_scale).log2().<span class="built_in">round</span>())</span><br><span class="line">            possible_scales.append(scale)</span><br><span class="line">        <span class="keyword">assert</span> possible_scales[<span class="number">0</span>] == possible_scales[<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">return</span> possible_scales[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">setup_scales</span>(<span class="params">self, features, image_shapes</span>):</span><br><span class="line">        <span class="comment"># type: (<span class="type">List</span>[Tensor], <span class="type">List</span>[<span class="type">Tuple</span>[<span class="built_in">int</span>, <span class="built_in">int</span>]])</span></span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">len</span>(image_shapes) != <span class="number">0</span></span><br><span class="line">        max_x = <span class="number">0</span></span><br><span class="line">        max_y = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> shape <span class="keyword">in</span> image_shapes:</span><br><span class="line">            max_x = <span class="built_in">max</span>(shape[<span class="number">0</span>], max_x)</span><br><span class="line">            max_y = <span class="built_in">max</span>(shape[<span class="number">1</span>], max_y)</span><br><span class="line">        original_input_shape = (max_x, max_y)</span><br><span class="line"></span><br><span class="line">        scales = [self.infer_scale(feat, original_input_shape) <span class="keyword">for</span> feat <span class="keyword">in</span> features]</span><br><span class="line">        <span class="comment"># get the levels in the feature map by leveraging the fact that the network always</span></span><br><span class="line">        <span class="comment"># downsamples by a factor of 2 at each level.</span></span><br><span class="line">        lvl_min = -torch.log2(torch.tensor(scales[<span class="number">0</span>], dtype=torch.float32)).item()</span><br><span class="line">        lvl_max = -torch.log2(torch.tensor(scales[-<span class="number">1</span>], dtype=torch.float32)).item()</span><br><span class="line">        self.scales = scales</span><br><span class="line">        self.map_levels = initLevelMapper(<span class="built_in">int</span>(lvl_min), <span class="built_in">int</span>(lvl_max))</span><br></pre></td></tr></table></figure>

<p>首先对每个feature_map输入的image的 下采样倍率scale进行计算，其中 infer_scale 函数采用如下的近似公式：</p>
<img src="/2023/07/26/FasterRCNN/image-20230726100756995.png" class="" title="image-20230726100756995">

<p>该公式相当于做了一个简单的映射，将不同的 feature_map 与 image 大小比映射到附近的尺度：</p>
<img src="/2023/07/26/FasterRCNN/image-20230726100815891.png" class="" title="image-20230726100815891">

<p>对于fasterRCNN实际值为[1&#x2F;4, 1&#x2F;8, 1&#x2F;16, 1&#x2F;32]</p>
<p>之后设置 lvl_min&#x3D;2 和 lvl_max&#x3D;5：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">lvl_min = -torch.log2(torch.tensor(scales[<span class="number">0</span>], dtype=torch.float32)).item()</span><br><span class="line">lvl_max = -torch.log2(torch.tensor(scales[-<span class="number">1</span>], dtype=torch.float32)).item()</span><br></pre></td></tr></table></figure>

<p>接着使用 FPN 原文中的公式计算 box 所在 anchor（其中 k0&#x3D;4 ， wℎ 为 box 面积）</p>
<img src="/2023/07/26/FasterRCNN/image-20230726101022738.png" class="" title="image-20230726101022738">

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LevelMapper</span>(<span class="title class_ inherited__">object</span>)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, k_min, k_max, canonical_scale=<span class="number">224</span>, canonical_level=<span class="number">4</span>, eps=<span class="number">1e-6</span></span>):</span><br><span class="line">        self.k_min = k_min          <span class="comment"># lvl_min=2</span></span><br><span class="line">        self.k_max = k_max          <span class="comment"># lvl_max=5</span></span><br><span class="line">        self.s0 = canonical_scale   <span class="comment"># 224</span></span><br><span class="line">        self.lvl0 = canonical_level <span class="comment"># 4</span></span><br><span class="line">        self.eps = eps</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, boxlists</span>):</span><br><span class="line">        s = torch.sqrt(torch.cat([box_area(boxlist) <span class="keyword">for</span> boxlist <span class="keyword">in</span> boxlists]))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Eqn.(1) in FPN paper</span></span><br><span class="line">        target_lvls = torch.floor(self.lvl0 + torch.log2(s / self.s0) + torch.tensor(self.eps, dtype=s.dtype))</span><br><span class="line">        target_lvls = torch.clamp(target_lvls, <span class="built_in">min</span>=self.k_min, <span class="built_in">max</span>=self.k_max)</span><br><span class="line">        <span class="keyword">return</span> (target_lvls.to(torch.int64) - self.k_min).to(torch.int64)</span><br></pre></td></tr></table></figure>

<p>其中 torch.clamp(input, min, max) → Tensor 函数的作用是截断，防止越界：</p>
<img src="/2023/07/26/FasterRCNN/image-20230726101103501.png" class="" title="image-20230726101103501">

<p> 可以看到，通过 LevelMapper 类将不同大小的 box 定位到某个 feature_map，如下图。之后就是按照图11中的流程进行 roi_pooling 操作。</p>
<img src="/2023/07/26/FasterRCNN/image-20230726101138226.png" class="" title="image-20230726101138226">

<h3 id="2-4-2-roi-pooling"><a href="#2-4-2-roi-pooling" class="headerlink" title="2.4.2 roi_pooling"></a>2.4.2 roi_pooling</h3><p>在确定 proposal box 所属 FPN 中哪个 feature_map 之后，接着来看 MultiScaleRoIAlign 如何进行 roi_pooling 操作：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MultiScaleRoIAlign</span>(nn.Module):</span><br><span class="line">   ......</span><br><span class="line"></span><br><span class="line">   <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, boxes, image_shapes</span>):</span><br><span class="line">        <span class="comment"># type: (<span class="type">Dict</span>[<span class="built_in">str</span>, Tensor], <span class="type">List</span>[Tensor], <span class="type">List</span>[<span class="type">Tuple</span>[<span class="built_in">int</span>, <span class="built_in">int</span>]]) -&gt; Tensor</span></span><br><span class="line">        x_filtered = []</span><br><span class="line">        <span class="keyword">for</span> k, v <span class="keyword">in</span> x.items():</span><br><span class="line">            <span class="keyword">if</span> k <span class="keyword">in</span> self.featmap_names:</span><br><span class="line">                x_filtered.append(v)</span><br><span class="line">        num_levels = <span class="built_in">len</span>(x_filtered)</span><br><span class="line">        rois = self.convert_to_roi_format(boxes)</span><br><span class="line">        <span class="keyword">if</span> self.scales <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            self.setup_scales(x_filtered, image_shapes)</span><br><span class="line"></span><br><span class="line">        scales = self.scales</span><br><span class="line">        <span class="keyword">assert</span> scales <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 没有 FPN 时，只有1/32的最后一个feature_map进行roi_pooling</span></span><br><span class="line">        <span class="keyword">if</span> num_levels == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> roi_align(</span><br><span class="line">                x_filtered[<span class="number">0</span>], rois,</span><br><span class="line">                output_size=self.output_size,</span><br><span class="line">                spatial_scale=scales[<span class="number">0</span>],</span><br><span class="line">                sampling_ratio=self.sampling_ratio</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 有 FPN 时，有4个feature_map进行roi_pooling</span></span><br><span class="line">        <span class="comment"># 首先按照</span></span><br><span class="line">        mapper = self.map_levels</span><br><span class="line">        <span class="keyword">assert</span> mapper <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        levels = mapper(boxes)</span><br><span class="line"></span><br><span class="line">        num_rois = <span class="built_in">len</span>(rois)</span><br><span class="line">        num_channels = x_filtered[<span class="number">0</span>].shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        dtype, device = x_filtered[<span class="number">0</span>].dtype, x_filtered[<span class="number">0</span>].device</span><br><span class="line">        result = torch.zeros(</span><br><span class="line">            (num_rois, num_channels,) + self.output_size,</span><br><span class="line">            dtype=dtype,</span><br><span class="line">            device=device,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        tracing_results = []</span><br><span class="line">        <span class="keyword">for</span> level, (per_level_feature, scale) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(x_filtered, scales)):</span><br><span class="line">            idx_in_level = torch.nonzero(levels == level).squeeze(<span class="number">1</span>)</span><br><span class="line">            rois_per_level = rois[idx_in_level]</span><br><span class="line"></span><br><span class="line">            result_idx_in_level = roi_align(</span><br><span class="line">                per_level_feature, rois_per_level,</span><br><span class="line">                output_size=self.output_size,</span><br><span class="line">                spatial_scale=scale, sampling_ratio=self.sampling_ratio)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> torchvision._is_tracing():</span><br><span class="line">                tracing_results.append(result_idx_in_level.to(dtype))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                result[idx_in_level] = result_idx_in_level</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> torchvision._is_tracing():</span><br><span class="line">            result = _onnx_merge_levels(levels, tracing_results)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure>

<p>在 MultiScaleRoIAlign.forward(…) 函数可以看到：</p>
<ul>
<li><p>没有 FPN 时，只有1&#x2F;32的最后一个 feature_map 进行 roi_pooling</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> num_levels == <span class="number">1</span>:</span><br><span class="line">       <span class="keyword">return</span> roi_align(</span><br><span class="line">           x_filtered[<span class="number">0</span>], rois,</span><br><span class="line">           output_size=self.output_size,</span><br><span class="line">           spatial_scale=scales[<span class="number">0</span>],</span><br><span class="line">           sampling_ratio=self.sampling_ratio</span><br><span class="line">       )</span><br></pre></td></tr></table></figure>
</li>
<li><p>有 FPN 时，有4个 [1&#x2F;4,1&#x2F;8,1&#x2F;16,1&#x2F;32] 的 feature maps 参加计算。首先计算每个每个 box 所属哪个 feature map ，再在所属 feature map 进行 roi_pooling</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 首先计算每个每个 box 所属哪个 feature map</span></span><br><span class="line">       levels = mapper(boxes) </span><br><span class="line">       ......</span><br><span class="line">  </span><br><span class="line">       <span class="comment"># 再在所属  feature map 进行 roi_pooling</span></span><br><span class="line">       <span class="comment"># 即 idx_in_level = torch.nonzero(levels == level).squeeze(1)</span></span><br><span class="line">       <span class="keyword">for</span> level, (per_level_feature, scale) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(x_filtered, scales)):</span><br><span class="line">           idx_in_level = torch.nonzero(levels == level).squeeze(<span class="number">1</span>)</span><br><span class="line">           rois_per_level = rois[idx_in_level]</span><br><span class="line">  </span><br><span class="line">           result_idx_in_level = roi_align(</span><br><span class="line">               per_level_feature, rois_per_level,</span><br><span class="line">               output_size=self.output_size,</span><br><span class="line">               spatial_scale=scale, sampling_ratio=self.sampling_ratio)</span><br></pre></td></tr></table></figure></li>
</ul>
<p>之后就获得了所谓的 7x7 特征（在 FasterRCNN.<strong>init</strong>(…) 中设置了 output_size&#x3D;7）。需要说明，原始 FasterRCNN 应该是使用 roi_pooling，但是这里使用 roi_align 代替以提升检测器性能。</p>
<p>对于 torchvision.ops.roi_align 函数输入的参数，分别为：</p>
<ul>
<li>per_level_feature 代表 FPN 输出的某一 feature_map</li>
<li>rois_per_level 为该特征 feature_map 对应的所有 proposal boxes（之前计算 level得到）</li>
<li>output_size&#x3D;7 代表输出为 7x7</li>
<li>spatial_scale 代表特征 feature_map 相对输入 image 的下采样尺度（如 1&#x2F;4，1&#x2F;8，…）</li>
<li>sampling_ratio 为 roi_align 采样率，有兴趣的读者请自行查阅 MaskRCNN 文章</li>
</ul>
<h3 id="2-4-3-分类回归信息"><a href="#2-4-3-分类回归信息" class="headerlink" title="2.4.3 分类回归信息"></a>2.4.3 分类回归信息</h3><p>接下来就是将特征转为最后针对 box 的类别信息（如人、猫、狗、车）和进一步的框回归信息。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TwoMLPHead</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, representation_size</span>):</span><br><span class="line">        <span class="built_in">super</span>(TwoMLPHead, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.fc6 = nn.Linear(in_channels, representation_size)</span><br><span class="line">        self.fc7 = nn.Linear(representation_size, representation_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = x.flatten(start_dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        x = F.relu(self.fc6(x))</span><br><span class="line">        x = F.relu(self.fc7(x))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">FastRCNNPredictor</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, num_classes</span>):</span><br><span class="line">        <span class="built_in">super</span>(FastRCNNPredictor, self).__init__()</span><br><span class="line">        self.cls_score = nn.Linear(in_channels, num_classes)</span><br><span class="line">        self.bbox_pred = nn.Linear(in_channels, num_classes * <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">if</span> x.dim() == <span class="number">4</span>:</span><br><span class="line">            <span class="keyword">assert</span> <span class="built_in">list</span>(x.shape[<span class="number">2</span>:]) == [<span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line">        x = x.flatten(start_dim=<span class="number">1</span>)</span><br><span class="line">        scores = self.cls_score(x)</span><br><span class="line">        bbox_deltas = self.bbox_pred(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> scores, bbox_deltas</span><br></pre></td></tr></table></figure>

<p>首先 TwoMLPHead 将 7x7 特征经过两个全连接层转为 1024，然后 FastRCNNPredictor 将每个 box 对应的 1024 维特征转为 cls_score 和 bbox_pred :</p>
<img src="/2023/07/26/FasterRCNN/image-20230726101709041.png" class="" title="image-20230726101709041">

<p>显然 cls_score 后接 softmax 即为类别概率，可以确定 box 的类别；在确定类别后，在 bbox_pred 中对应类别的 （dx, dy, dw, dh）4个值即为第二次 bounding box regression 需要的4个偏移值。</p>
<p><strong>简单的说，带有FPN的FasterRCNN网络结构可以用下图表示：</strong></p>
<img src="/2023/07/26/FasterRCNN/image-20230726101751541.png" class="" title="image-20230726101751541">

<h1 id="3-关于训练"><a href="#3-关于训练" class="headerlink" title="3. 关于训练"></a>3. 关于训练</h1><p>FasterRCNN模型在两处地方有损失函数：</p>
<ul>
<li>在 RegionProposalNetwork 类，需要判别 anchor 中是否包含目标从而生成 proposals，这里需要计算 loss</li>
<li>在 RoIHeads 类，对 roi_pooling 后的全连接生成的 cls_score 和 bbox_pred 进行训练，也需要计算 loss</li>
</ul>
<h2 id="3-1-assign-targets-to-anchors"><a href="#3-1-assign-targets-to-anchors" class="headerlink" title="3.1 assign_targets_to_anchors"></a>3.1 assign_targets_to_anchors</h2><p>首先来看 RegionProposalNetwork 类中的 assign_targets_to_anchors 函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">assign_targets_to_anchors</span>(<span class="params">self, anchors, targets</span>):</span><br><span class="line">    <span class="comment"># type: (<span class="type">List</span>[Tensor], <span class="type">List</span>[<span class="type">Dict</span>[<span class="built_in">str</span>, Tensor]])</span></span><br><span class="line">    labels = []</span><br><span class="line">    matched_gt_boxes = []</span><br><span class="line">    <span class="keyword">for</span> anchors_per_image, targets_per_image <span class="keyword">in</span> <span class="built_in">zip</span>(anchors, targets):</span><br><span class="line">        gt_boxes = targets_per_image[<span class="string">&quot;boxes&quot;</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> gt_boxes.numel() == <span class="number">0</span>:</span><br><span class="line">            <span class="comment"># Background image (negative example)</span></span><br><span class="line">            device = anchors_per_image.device</span><br><span class="line">            matched_gt_boxes_per_image = torch.zeros(anchors_per_image.shape, dtype=torch.float32, device=device)</span><br><span class="line">            labels_per_image = torch.zeros((anchors_per_image.shape[<span class="number">0</span>],), dtype=torch.float32, device=device)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            match_quality_matrix = box_ops.box_iou(gt_boxes, anchors_per_image)</span><br><span class="line">            matched_idxs = self.proposal_matcher(match_quality_matrix)</span><br><span class="line">            <span class="comment"># get the targets corresponding GT for each proposal</span></span><br><span class="line">            <span class="comment"># NB: need to clamp the indices because we can have a single</span></span><br><span class="line">            <span class="comment"># GT in the image, and matched_idxs can be -2, which goes</span></span><br><span class="line">            <span class="comment"># out of bounds</span></span><br><span class="line">            matched_gt_boxes_per_image = gt_boxes[matched_idxs.clamp(<span class="built_in">min</span>=<span class="number">0</span>)]</span><br><span class="line"></span><br><span class="line">            labels_per_image = matched_idxs &gt;= <span class="number">0</span></span><br><span class="line">            labels_per_image = labels_per_image.to(dtype=torch.float32)</span><br><span class="line">            <span class="comment"># Background (negative examples)</span></span><br><span class="line">            bg_indices = matched_idxs == self.proposal_matcher.BELOW_LOW_THRESHOLD</span><br><span class="line">            labels_per_image[bg_indices] = torch.tensor(<span class="number">0.0</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># discard indices that are between thresholds</span></span><br><span class="line">            inds_to_discard = matched_idxs == self.proposal_matcher.BETWEEN_THRESHOLDS</span><br><span class="line">            labels_per_image[inds_to_discard] = torch.tensor(-<span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line">        labels.append(labels_per_image)</span><br><span class="line">        matched_gt_boxes.append(matched_gt_boxes_per_image)</span><br><span class="line">    <span class="keyword">return</span> labels, matched_gt_boxes</span><br></pre></td></tr></table></figure>

<p>当图像中没有 gt_boxes 时，设置所有 anchor 都为 background（即 label 为 0）：</p>
<p>当图像中有 gt_boxes 时，计算 anchor 与 gt_box 的 IOU：</p>
<ul>
<li><p>选择 IOU &lt; 0.3 的 anchor 为 background，标签为 0</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">labels_per_image[bg_indices] = torch.tensor(<span class="number">0.0</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>选择 IOU &gt; 0.7 的 anchor 为 foreground，标签为 1</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">labels_per_image = matched_idxs &gt;= <span class="number">0</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>忽略 0.3 &lt; IOU &lt; 0.7 的 anchor，不参与训练</p>
</li>
</ul>
<p>从 FasterRCNN 类的 <strong>init</strong> 函数默认参数就可以清晰的看到这一点：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rpn_fg_iou_thresh=<span class="number">0.7</span>, rpn_bg_iou_thresh=<span class="number">0.3</span>,</span><br></pre></td></tr></table></figure>

<h2 id="3-2-assign-targets-to-proposals"><a href="#3-2-assign-targets-to-proposals" class="headerlink" title="3.2 assign_targets_to_proposals"></a>3.2 assign_targets_to_proposals</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">assign_targets_to_proposals</span>(<span class="params">self, proposals, gt_boxes, gt_labels</span>):</span><br><span class="line">    <span class="comment"># type: (<span class="type">List</span>[Tensor], <span class="type">List</span>[Tensor], <span class="type">List</span>[Tensor])</span></span><br><span class="line">    matched_idxs = []</span><br><span class="line">    labels = []</span><br><span class="line">    <span class="keyword">for</span> proposals_in_image, gt_boxes_in_image, gt_labels_in_image <span class="keyword">in</span> <span class="built_in">zip</span>(proposals, gt_boxes, gt_labels):</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> gt_boxes_in_image.numel() == <span class="number">0</span>:</span><br><span class="line">            <span class="comment"># Background image</span></span><br><span class="line">            device = proposals_in_image.device</span><br><span class="line">            clamped_matched_idxs_in_image = torch.zeros(</span><br><span class="line">                (proposals_in_image.shape[<span class="number">0</span>],), dtype=torch.int64, device=device</span><br><span class="line">            )</span><br><span class="line">            labels_in_image = torch.zeros(</span><br><span class="line">                (proposals_in_image.shape[<span class="number">0</span>],), dtype=torch.int64, device=device</span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment">#  set to self.box_similarity when https://github.com/pytorch/pytorch/issues/27495 lands</span></span><br><span class="line">            match_quality_matrix = box_ops.box_iou(gt_boxes_in_image, proposals_in_image)</span><br><span class="line">            matched_idxs_in_image = self.proposal_matcher(match_quality_matrix)</span><br><span class="line"></span><br><span class="line">            clamped_matched_idxs_in_image = matched_idxs_in_image.clamp(<span class="built_in">min</span>=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">            labels_in_image = gt_labels_in_image[clamped_matched_idxs_in_image]</span><br><span class="line">            labels_in_image = labels_in_image.to(dtype=torch.int64)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Label background (below the low threshold)</span></span><br><span class="line">            bg_inds = matched_idxs_in_image == self.proposal_matcher.BELOW_LOW_THRESHOLD</span><br><span class="line">            labels_in_image[bg_inds] = torch.tensor(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Label ignore proposals (between low and high thresholds)</span></span><br><span class="line">            ignore_inds = matched_idxs_in_image == self.proposal_matcher.BETWEEN_THRESHOLDS</span><br><span class="line">            labels_in_image[ignore_inds] = torch.tensor(-<span class="number">1</span>)  <span class="comment"># -1 is ignored by sampler</span></span><br><span class="line"></span><br><span class="line">        matched_idxs.append(clamped_matched_idxs_in_image)</span><br><span class="line">        labels.append(labels_in_image)</span><br><span class="line">    <span class="keyword">return</span> matched_idxs, labels</span><br></pre></td></tr></table></figure>

<p>与 assign_targets_to_anchors 不同，该函数设置：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">box_fg_iou_thresh=<span class="number">0.5</span>, box_bg_iou_thresh=<span class="number">0.5</span>,</span><br></pre></td></tr></table></figure>

<ul>
<li><p>IOU &gt; 0.5 的 proposal 为 foreground，标签为对应的 class_id</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">labels_in_image = gt_labels_in_image[clamped_matched_idxs_in_image]</span><br></pre></td></tr></table></figure>

<p>这里与上面不同：RegionProposalNetwork 只需要判断 anchor 是否有目标，正类别为1；RoIHeads 需要判断 proposal 的具体类别，所以正类别为具体的 class_id。</p>
</li>
<li><p>IOU &lt; 0.5 的为 background，标签为 0</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">labels_in_image[bg_inds] = torch.tensor(<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
</li>
<li></li>
</ul>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://blog.icathianrain.me">IcathianRain</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://blog.icathianrain.me/2023/07/26/FasterRCNN/">https://blog.icathianrain.me/2023/07/26/FasterRCNN/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://blog.icathianrain.me" target="_blank">IcathianRain's Blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/AI/">AI</a><a class="post-meta__tags" href="/tags/FasterRCNN/">FasterRCNN</a></div><div class="post_share"><div class="social-share" data-image="/images/avatar.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2023/07/26/mmdetection/" title="mmdetection"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">mmdetection</div></div></a></div><div class="next-post pull-right"><a href="/2023/07/24/cmake-tutorial/" title="cmake_tutorial"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">cmake_tutorial</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2023/07/26/mmdetection/" title="mmdetection"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-07-26</div><div class="title">mmdetection</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/images/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">IcathianRain</div><div class="author-info__description">今夜，我是天选，也是唯一。I was gonna be that one in a million.</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">6</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">7</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">2</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/Icathian-Rain"><i class="fab fa-github"></i><span>关注</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/Icathian-Rain" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="https://space.bilibili.com/47310416" target="_blank" title="Bilibili"><i class="iconfont icon-bilibili" style="color: #20B0E3;"></i></a><a class="social-icon" href="mailto:2205794866@qq.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Pytorch%E5%AE%98%E6%96%B9FasterRCNN"><span class="toc-text">Pytorch官方FasterRCNN</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#0-%E4%BB%A3%E7%A0%81%E7%BB%93%E6%9E%84"><span class="toc-text">0. 代码结构</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#1-GeneralizedRCNN"><span class="toc-text">1. GeneralizedRCNN</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-1-transform"><span class="toc-text">1.1 transform</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-2-backbone-rpn-roi-heads"><span class="toc-text">1.2 backbone+rpn+roi_heads</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-FasterRCNN"><span class="toc-text">2. FasterRCNN</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-1-transform-%E6%8E%A5%E5%8F%A3"><span class="toc-text">2.1 transform 接口</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2-backbone-%E6%8E%A5%E5%8F%A3"><span class="toc-text">2.2 backbone 接口</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-1-ResNet50"><span class="toc-text">2.2.1 ResNet50</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-2-FPN-Feature-Pyramid-Network"><span class="toc-text">2.2.2 FPN (Feature Pyramid Network)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E5%8A%A8%E6%9C%BA"><span class="toc-text">1. 动机</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E7%BB%93%E6%9E%84"><span class="toc-text">2.结构</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E7%89%B9%E5%BE%81%E9%87%91%E5%AD%97%E5%A1%94"><span class="toc-text">3. 特征金字塔</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-3-rpn"><span class="toc-text">2.3 rpn</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-1-rpn-anchor-generator"><span class="toc-text">2.3.1 rpn_anchor_generator</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-2-RPNHead"><span class="toc-text">2.3.2 RPNHead</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-3-RegionProposalNetwork"><span class="toc-text">2.3.3 RegionProposalNetwork</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-4-roi-heads"><span class="toc-text">2.4 roi_heads</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-1-%E8%AE%A1%E7%AE%97box%E5%BD%92%E5%B1%9E%E7%9A%84feature-map"><span class="toc-text">2.4.1 计算box归属的feature_map</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-2-roi-pooling"><span class="toc-text">2.4.2 roi_pooling</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-3-%E5%88%86%E7%B1%BB%E5%9B%9E%E5%BD%92%E4%BF%A1%E6%81%AF"><span class="toc-text">2.4.3 分类回归信息</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3-%E5%85%B3%E4%BA%8E%E8%AE%AD%E7%BB%83"><span class="toc-text">3. 关于训练</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1-assign-targets-to-anchors"><span class="toc-text">3.1 assign_targets_to_anchors</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2-assign-targets-to-proposals"><span class="toc-text">3.2 assign_targets_to_proposals</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/07/30/music/" title="Blog 音乐页创建问题">Blog 音乐页创建问题</a><time datetime="2023-07-30T09:58:36.000Z" title="发表于 2023-07-30 09:58:36">2023-07-30</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/07/26/mmdetection/" title="mmdetection">mmdetection</a><time datetime="2023-07-26T16:03:28.000Z" title="发表于 2023-07-26 16:03:28">2023-07-26</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/07/26/FasterRCNN/" title="Faster RCNN 源码解析">Faster RCNN 源码解析</a><time datetime="2023-07-26T10:46:33.000Z" title="发表于 2023-07-26 10:46:33">2023-07-26</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/07/24/cmake-tutorial/" title="cmake_tutorial">cmake_tutorial</a><time datetime="2023-07-24T09:46:33.000Z" title="发表于 2023-07-24 09:46:33">2023-07-24</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/07/24/vmware-tools/" title="vmware_tools">vmware_tools</a><time datetime="2023-07-24T09:46:33.000Z" title="发表于 2023-07-24 09:46:33">2023-07-24</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2023 By IcathianRain</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>